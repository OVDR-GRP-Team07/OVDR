<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>GRP Project Overview</title>
    <style>
        body {
            font-family: "Arial", sans-serif;
            margin: 40px;
            line-height: 1.6;
        }
        hr {
            margin: 20px 0;
        }
        h2, h3 {
            color: #2e6c80;
        }
        ul, ol {
            margin-left: 20px;
        }
        a {
            color: #2e6c80;
        }
    </style>
</head>
<body>

<h2>GRP Project: Online Virtual Dressing Room</h2>
<hr>

<h3>Project Title</h3>
<p><strong>[P2024-16]</strong> Online Virtual Dressing Room with Advanced Try-On and Clothing Retrieval Features</p>

<h3>Team</h3>
<ul>
    <li><strong>Team Name:</strong> TEAM2024.07</li>
    <li><strong>Supervisor:</strong> Dr. Qian Zhang</li>
</ul>

<h3>Project Objectives</h3>
<ul>
    <li>Provide a photorealistic virtual try-on experience through 2D garment fitting (StableVITON).</li>
    <li>Enable semantic clothing search and recommendations using CLIP and similarity scoring.</li>
    <li>Allow users to manage closets, try on outfits, and save combinations.</li>
    <li>Support image caption generation to improve retrieval and dataset quality.</li>
</ul>

<h3>Dataset</h3>
<h3>Dataset</h3>
<p>
Our team developed a categorized dataset of clothing items: <strong>tops</strong>, <strong>bottoms</strong>, and <strong>dresses</strong>.  
Each item includes flat images, model try-on visuals, and <strong>captioned text descriptions</strong> generated using Alibaba's large language model <strong>Qwen (通义千问)</strong> to support text-based search and semantic recommendation.
</p>


<h3>Retrieval & Recommendation System</h3>
<p>
We use the <strong>CLIP (Contrastive Language–Image Pre-training)</strong> model to encode both images and user search queries into the same vector space.  
Text-based search is implemented by comparing the <strong>cosine similarity</strong> between the query vector and all clothing image vectors.  
Similarly, <strong>click-based recommendation</strong> uses a precomputed similarity matrix to find and recommend visually or semantically similar clothing items.
</p>

<h3>Database</h3>
<p>
The backend uses a <strong>MySQL relational database</strong> to manage users, uploaded images, clothing metadata, closet selections, and try-on combinations.  
The database schema is defined in `backend/database/ovdr_structure.sql`, with sample data in `ovdr_data_only.sql`.  
An ERD is available as `ERD.png` in the same directory.
</p>

<h3>Technologies Used</h3>
<ul>
    <li>Frontend: React.js</li>
    <li>Backend: Flask (Python), SQLAlchemy, MySQL</li>
    <li>Models: StableVITON (try-on), CLIP (search & similarity), Qwen (captioning)</li>
    <li>Other: HTML, CSS, JavaScript, Node.js</li>
</ul>

<h3>Key Deliverables</h3>
<ul>
    <li>✔️ Full-stack virtual dressing room web app</li>
    <li>✔️ CLIP-powered retrieval and recommendation engine</li>
    <li>✔️ User closet, outfit history, combination saving</li>
    <li>✔️ Integrated dataset with captions</li>
    <li>✔️ RESTful API with documentation</li>
</ul>

<h3>Try It Out</h3>
<ul>
    <li>Live site: <a href="http://cslinux.nottingham.edu.cn/~Team202407/" target="_blank">Team Web Portal</a></li>
    <li>GitLab: <a href="https://csprojects.nottingham.edu.cn/grp-team07-gitlab/grp-team07-gitlab-work" target="_blank">https://csprojects.nottingham.edu.cn/grp-team07-gitlab/grp-team07-gitlab-work</a></li>
    <li>GitHub: <a href="https://github.com/OVDR-GRP-Team07/OVDR" target="_blank">https://github.com/OVDR-GRP-Team07/OVDR</a></li>
</ul>

</body>
</html>
